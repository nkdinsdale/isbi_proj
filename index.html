<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title><script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-size:32px;
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Omni-supervised domain adversarial training for white matter hyperintensity segmentation the UK Biobank</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Omni-supervised domain adversarial training" />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Omni-supervised domain adversarial training for <br> white matter hyperintensity segmentation the UK Biobank</span>
		<br>
		<br>
		<center><font size="+2"><i> ISBI 2022 (Oral Presentation)</font></center>
		<table align=center width=600px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href="https://www.ndcn.ox.ac.uk/team/vaanathi-sundaresan">Vaanathi Sundaresan </a><sup>1</sup></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href="https://nkdinsdale.github.io/nkdinsdale/">Nicola Dinsdale</a><sup>1</sup></span>
						</center>
					</td>

				<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://www.ndcn.ox.ac.uk/team/mark-jenkinson">Mark Jenkinson</a><sup>1,</sup><sup>2,</sup><sup>3</sup></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href="https://www.neuroscience.ox.ac.uk/research-directory/ludovica-griffanti"> Ludovica Griffanti </a><sup>1</sup></span>
						</center>
					</td>
				</tr>
			</table>

					<br>
			<table align=center width=800px>
				<tr>
					<td align=center width=800px>
						<center>
							<span style="font-size:12px"><sup>1</sup>Wellcome Centre for Integrative Neuroimaging, University of Oxford</a></span>
						</center>
					</td>
				</tr>
			</table>


			<table align=center width=800px>
				<tr>
					<td align=center width=800px>
						<center>
							<span style="font-size:12px"><sup>2</sup>Australian Institute for Machine Learning (AIML), University of Adelaide </a></span>
						</center>
					</td>
				</tr>
			</table>

			<table align=center width=800px>
				<tr>
					<td align=center width=800px>
						<center>
							<span style="font-size:12px"><sup>3</sup>South Australian Health and Medical Research Institute (SAHMRI) </a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>


			<table align=center width=700px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">[Paper] (Coming soon)</span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/v-sundaresan/omnisup_agepred_semidann'>[GitHub]</a></span><br>
						</center>
					</td>
										<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href='https://git.fmrib.ox.ac.uk/vaanathi/truenet'>[TrueNet Code]</a></span><br>
						</center>
					</td>
										</td>
										<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/nkdinsdale/LearningPatternsofAgeing'>[Age Prediction Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=1000px>
			<tr>
				<td width=800px>
					<center>
						<img class="round" style="width:800px" src="./resources/wmh_large.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
White matter hyperintensities (WMHs, or lesions) appear as
hyperintense, localized regions on T2-weighted and FLAIR
brain MR images. The heterogeneity in lesion characteristics
due to subject-level (e.g., local intensity/contrast) and
population-level (e.g., demographic, scanner-related)
variations make their segmentation highly challenging. Here,
we propose a framework for adapting a state-of-the-art WMH
segmentation method with high accuracy from a small,
labeled source data (MICCAI WMH segmentation challenge
2017 training data) to a larger dataset such as the UK Biobank
without the need of additional manual training labels, using
domain adversarial training with omni-supervised learning.
Given the well-known association of WMHs with age, the
proposed method uses a multi-tasking model for learning
lesion segmentation, domain adaptation and age prediction
simultaneously. On a subset of the UK Biobank dataset, the
proposed method achieves a lesion-level recall, lesion-level
F1-measure and Dice overlap value of 0.95, 0.65 and 0.84
respectively, when compared to values of 0.75, 0.49 and 0.80
obtained from the pretrained state-of-the-art baseline method.

			</td>
		</tr>
	</table>
	<br>

	<center><h1>Pipeline</h1></center>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					We combine domain adaptation and omni supervised learning to significantly improve the segmentation performance on white matter hyperintensities. Data from the MICCAI challenge dataset was used as the source data, and unlabelled UK Biobank data was the target. The data points used within the omnisupervised framework were selected using Age Prediction Deltas, as age is known to correlate with the presence of WMHs.
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/Network_architecture.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
				A network composed of three parts is trained: 1) WMH segmentation network (TrueNet) 2) Domain Prediction 3) Age Predictor
				</td>
			</tr>
		</center>
	</table>


	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/v-sundaresan/omnisup_agepred_semidann'>[GitHub]</a>
			</center>
		</span>
	</table>
	<br>

		<center><h1>Results</h1></center>
	<center><font size="+2"><i> Ablation Study </font></center>
		<tr>
			<td width=1000px>
        		<center>
        			<img class="round" style="width:800px" src="./resources/table.png"/>
        		</center>
        	</td>
		</tr>
	<br>

	</table>

		<tr>
    	<td>
			<center>
				<font size="-0.2"> The approach leads to a significant improvement across metrics, especially lesion level recall, showing significant improvement in detecting small lesions.</font>
    		</center>

		</td>
    </tr>


	<br>
	<br>
	<center><font size="+2"><i> Visual Results </font></center>


	<br>
			<tr>
			<td width=600px>
        		<center>
        			<img class="round" style="width:800px" src="./resources/visual_results.png"/>
        		</center>
        	</td>
		</tr>

		<tr>
    	<td>
			<center>
				<font size="-0.2"> Sample segmentation results shown for (a) Baseline pretrained on source, (b) unsup-DANN, (c) ssDANN-OL and (d)
ssDANN-OL-age pred. <br> </br>Yellow, blue and red voxels indicate true
positive, false negative and false positive voxels respectively.</font>
    		</center>

		</td>
    </tr>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=300px>
				<left>
					<center><h1>Acknowledgements</h1></center>
				This research is funded by the Engineering and Physical
				Sciences Research Council (EPSRC) and Medical Research
					Council (MRC) [EP/L016052/1] and Wellcome Centre for
Integrative Neuroimaging, which has core funding from the
Wellcome Trust [203139/Z/16/Z]. The computational aspects
of this research were funded from National Institute for
Health Research (NIHR) Oxford Biomedical Research
Centre (BRC) with additional support from the Wellcome
Trust Core Award [203141/Z/16/Z]. The authors
acknowledge the MRC Dementias Platform UK
(MR/L023784/2), the NIHR Oxford Health BRC and the
NIHR Oxford BRC. The UK Biobank data was obtained
under app. number 8107. We are grateful to UK Biobank for
making the resource data available and are extremely grateful
to all UK Biobank study participants.
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

